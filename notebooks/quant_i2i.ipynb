{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.distributed as dist\n",
    "import math\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.image_datasets import load_data_sde\n",
    "from guided_diffusion.script_util import (\n",
    "    NUM_CLASSES,\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    ")\n",
    "from torchvision import utils\n",
    "from easydict import EasyDict\n",
    "import PIL.Image as Image\n",
    "import torchvision\n",
    "import lpips\n",
    "from psnrssim import MetricI2I\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "device = 'cuda:5'\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MetricI2I(scope='', device=device)\n",
    "# TODO\n",
    "# SDEdit에서 input image (FFHQ)와 out image(MetFaces style image)의 L2, SSIM, PSNR재기 with last_step = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_src = EasyDict({'attention_resolutions':'16', \n",
    "        'batch_size':batch_size, \n",
    "        'channel_mult':'', \n",
    "        'class_cond':False, \n",
    "        'clip_denoised':True, \n",
    "        'diffusion_steps':1000, \n",
    "        'dropout':0.0, \n",
    "        'image_size':256, \n",
    "        'learn_sigma':True, \n",
    "        'model_path':'../models/ffhq_p2.pt', \n",
    "        'noise_schedule':'linear', \n",
    "        'num_channels':128, \n",
    "        'num_head_channels':64, \n",
    "        'num_heads':4, \n",
    "        'num_heads_upsample':-1, \n",
    "        'num_res_blocks':1, \n",
    "        'num_samples':10000, \n",
    "        'p2_gamma':0, \n",
    "        'p2_k':1, \n",
    "        'predict_xstart':False, \n",
    "        'resblock_updown':True, \n",
    "        'rescale_learned_sigmas':False,\n",
    "        'rescale_timesteps':False, \n",
    "        'sample_dir':'samples/metface_transfer_step_40_40k',\n",
    "        'timestep_respacing':'ddim40', \n",
    "        'use_checkpoint':False,\n",
    "        'use_ddim':True, \n",
    "        'use_fp16':True, \n",
    "        'use_kl':False, \n",
    "        'use_new_attention_order':False, \n",
    "        'use_scale_shift_norm':True,\n",
    "        'data_dir':''})\n",
    "        \n",
    "model_src, diffusion = create_model_and_diffusion(\n",
    "    **args_to_dict(args_src, model_and_diffusion_defaults().keys())\n",
    ")\n",
    "\n",
    "model_src.load_state_dict(\n",
    "    dist_util.load_state_dict(args_src.model_path, map_location=\"cpu\")\n",
    ")\n",
    "model_src.to(device)\n",
    "model_src.convert_to_fp16()\n",
    "model_src.eval()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data_sde(\n",
    "        data_dir='../FFHQ_partial/',\n",
    "        batch_size=args_src.batch_size,\n",
    "        image_size=args_src.image_size,\n",
    "        class_cond=args_src.class_cond,\n",
    "        deterministic=True\n",
    "    )\n",
    "        # data_dir='/mnt/raid/FFHQ_10k/',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open('log_SDEdit_AAHQ1400.txt', 'w')\n",
    "models = ['../models/ffhq_p2.pt',\n",
    "            '../logs/aahq_limited1400_distill_p2_0.2_3_aux_0.2_50/model030000.pt',\n",
    "            '../logs/aahq_limited1400_distill_p2_0.1_3_aux_0.3_50/model020000.pt',\n",
    "            '../logs/aahq_limited1400_distill_p2_0.1_3_aux_0.3_50/model010000.pt',\n",
    "            ]\n",
    "for model_path in models:\n",
    "    args_src = EasyDict({'attention_resolutions':'16', \n",
    "            'batch_size':batch_size, \n",
    "            'channel_mult':'', \n",
    "            'class_cond':False, \n",
    "            'clip_denoised':True, \n",
    "            'diffusion_steps':1000, \n",
    "            'dropout':0.0, \n",
    "            'image_size':256, \n",
    "            'learn_sigma':True, \n",
    "            'model_path':model_path, \n",
    "            'noise_schedule':'linear', \n",
    "            'num_channels':128, \n",
    "            'num_head_channels':64, \n",
    "            'num_heads':4, \n",
    "            'num_heads_upsample':-1, \n",
    "            'num_res_blocks':1, \n",
    "            'num_samples':10000, \n",
    "            'p2_gamma':0, \n",
    "            'p2_k':1, \n",
    "            'predict_xstart':False, \n",
    "            'resblock_updown':True, \n",
    "            'rescale_learned_sigmas':False,\n",
    "            'rescale_timesteps':False, \n",
    "            'sample_dir':'samples/metface_transfer_step_40_40k',\n",
    "            'timestep_respacing':'ddim40', \n",
    "            'use_checkpoint':False,\n",
    "            'use_ddim':True, \n",
    "            'use_fp16':True, \n",
    "            'use_kl':False, \n",
    "            'use_new_attention_order':False, \n",
    "            'use_scale_shift_norm':True,\n",
    "            'data_dir':''})\n",
    "            \n",
    "    model_src, diffusion = create_model_and_diffusion(\n",
    "        **args_to_dict(args_src, model_and_diffusion_defaults().keys())\n",
    "    )\n",
    "\n",
    "    model_src.load_state_dict(\n",
    "        dist_util.load_state_dict(args_src.model_path, map_location=\"cpu\")\n",
    "    )\n",
    "    model_src.to(device)\n",
    "    model_src.convert_to_fp16()\n",
    "    model_src.eval()\n",
    "    print('done')\n",
    "\n",
    "    model_kwargs = {}\n",
    "    sample_fn = (\n",
    "        diffusion.p_sample_loop if not args_src.use_ddim else diffusion.ddim_sample_loop\n",
    "    )\n",
    "    dist = []\n",
    "    t_end = 20\n",
    "\n",
    "    n_trial = 1\n",
    "    for n in range(1,n_trial+1):\n",
    "        metric.reset()\n",
    "        metric.scope = f'SDEdit {n}th trial'\n",
    "        with th.no_grad():\n",
    "            for idx, batch in enumerate(data):\n",
    "                img, cond = batch\n",
    "                img = img.to(device)\n",
    "                noise = th.randn_like(img)\n",
    "                t = th.tensor([t_end] * img.shape[0], device=device)\n",
    "                perturbed = diffusion.q_sample(img, t, noise=noise)\n",
    "                sample_src = sample_fn(\n",
    "                    model_src,\n",
    "                    (args_src.batch_size, 3, args_src.image_size, args_src.image_size),\n",
    "                    clip_denoised=args_src.clip_denoised,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    noise=perturbed,\n",
    "                    t_start=0,\n",
    "                    t_end=t_end\n",
    "                )\n",
    "                metric.update(img, sample_src)\n",
    "        \n",
    "        print(model_path)\n",
    "        print(model_path, file=f)\n",
    "        print(metric.print_metrics())\n",
    "        print(metric.print_metrics(), file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = ((img + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "pil_sam = ((sample_src + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "display(to_pil(pil_img[0]))\n",
    "display(to_pil(pil_sam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='../logs/metface_distill_14/model010000.pt'\n",
    "args_src = EasyDict({'attention_resolutions':'16', \n",
    "        'batch_size':batch_size, \n",
    "        'channel_mult':'', \n",
    "        'class_cond':False, \n",
    "        'clip_denoised':True, \n",
    "        'diffusion_steps':1000, \n",
    "        'dropout':0.0, \n",
    "        'image_size':256, \n",
    "        'learn_sigma':True, \n",
    "        'model_path':model_path, \n",
    "        'noise_schedule':'linear', \n",
    "        'num_channels':128, \n",
    "        'num_head_channels':64, \n",
    "        'num_heads':4, \n",
    "        'num_heads_upsample':-1, \n",
    "        'num_res_blocks':1, \n",
    "        'num_samples':10000, \n",
    "        'p2_gamma':0, \n",
    "        'p2_k':1, \n",
    "        'predict_xstart':False, \n",
    "        'resblock_updown':True, \n",
    "        'rescale_learned_sigmas':False,\n",
    "        'rescale_timesteps':False, \n",
    "        'sample_dir':'samples/metface_transfer_step_40_40k',\n",
    "        'timestep_respacing':'ddim40', \n",
    "        'use_checkpoint':False,\n",
    "        'use_ddim':True, \n",
    "        'use_fp16':True, \n",
    "        'use_kl':False, \n",
    "        'use_new_attention_order':False, \n",
    "        'use_scale_shift_norm':True,\n",
    "        'data_dir':''})\n",
    "        \n",
    "model_src, diffusion = create_model_and_diffusion(\n",
    "    **args_to_dict(args_src, model_and_diffusion_defaults().keys())\n",
    ")\n",
    "\n",
    "model_src.load_state_dict(\n",
    "    dist_util.load_state_dict(args_src.model_path, map_location=\"cpu\")\n",
    ")\n",
    "model_src.to(device)\n",
    "model_src.convert_to_fp16()\n",
    "model_src.eval()\n",
    "print('done')\n",
    "\n",
    "model_kwargs = {}\n",
    "sample_fn = (\n",
    "    diffusion.p_sample_loop if not args_src.use_ddim else diffusion.ddim_sample_loop\n",
    ")\n",
    "dist = []\n",
    "t_end = 20\n",
    "\n",
    "n_trial = 3\n",
    "for n in range(n_trial):\n",
    "    metric.reset()\n",
    "    metric.scope = f'SDEdit {n}th trial'\n",
    "    with th.no_grad():\n",
    "        for idx, batch in enumerate(data):\n",
    "            img, cond = batch\n",
    "            img = img.to(device)\n",
    "            noise = th.randn_like(img)\n",
    "            t = th.tensor([t_end] * img.shape[0], device=device)\n",
    "            perturbed = diffusion.q_sample(img, t, noise=noise)\n",
    "            sample_src = sample_fn(\n",
    "                model_src,\n",
    "                (args_src.batch_size, 3, args_src.image_size, args_src.image_size),\n",
    "                clip_denoised=args_src.clip_denoised,\n",
    "                model_kwargs=model_kwargs,\n",
    "                noise=perturbed,\n",
    "                t_start=0,\n",
    "                t_end=t_end\n",
    "            )\n",
    "            metric.update(img, sample_src)\n",
    "    \n",
    "    print(model_path)\n",
    "    print(metric.print_metrics())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ILVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resizer import Resizer\n",
    "\n",
    "# ILVR\n",
    "down_N = 16\n",
    "shape = (batch_size, 3, 256, 256)\n",
    "shape_d = (batch_size, 3, int(256 / down_N), int(256 / down_N))\n",
    "down = Resizer(shape, 1 / down_N).to(device)\n",
    "up = Resizer(shape_d, down_N).to(device)\n",
    "resizers = (down, up)\n",
    "\n",
    "sample_fn = (\n",
    "    diffusion.p_sample_loop if not args_src.use_ddim else diffusion.ddim_sample_loop\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resizer import Resizer\n",
    "\n",
    "f= open('log_ILVR.txt', 'w')\n",
    "models = ['../models/ffhq_p2.pt',\n",
    "            '../logs/metface_scratch_noglass/model060000.pt',\n",
    "            '../logs/metface_transfer_noglass/model060000.pt',\n",
    "            '../logs/metface_distill_3/model030000.pt',\n",
    "            '../logs/metface_distill_14/model010000.pt'\n",
    "            ]\n",
    "\n",
    "# ILVR\n",
    "down_N = 16\n",
    "shape = (batch_size, 3, 256, 256)\n",
    "shape_d = (batch_size, 3, int(256 / down_N), int(256 / down_N))\n",
    "down = Resizer(shape, 1 / down_N).to(device)\n",
    "up = Resizer(shape_d, down_N).to(device)\n",
    "resizers = (down, up)\n",
    "\n",
    "sample_fn = (\n",
    "    diffusion.p_sample_loop if not args_src.use_ddim else diffusion.ddim_sample_loop\n",
    ")\n",
    "\n",
    "for model_path in models:\n",
    "    args_src = EasyDict({'attention_resolutions':'16', \n",
    "            'batch_size':batch_size, \n",
    "            'channel_mult':'', \n",
    "            'class_cond':False, \n",
    "            'clip_denoised':True, \n",
    "            'diffusion_steps':1000, \n",
    "            'dropout':0.0, \n",
    "            'image_size':256, \n",
    "            'learn_sigma':True, \n",
    "            'model_path':model_path, \n",
    "            'noise_schedule':'linear', \n",
    "            'num_channels':128, \n",
    "            'num_head_channels':64, \n",
    "            'num_heads':4, \n",
    "            'num_heads_upsample':-1, \n",
    "            'num_res_blocks':1, \n",
    "            'num_samples':10000, \n",
    "            'p2_gamma':0, \n",
    "            'p2_k':1, \n",
    "            'predict_xstart':False, \n",
    "            'resblock_updown':True, \n",
    "            'rescale_learned_sigmas':False,\n",
    "            'rescale_timesteps':False, \n",
    "            'sample_dir':'samples/metface_transfer_step_40_40k',\n",
    "            'timestep_respacing':'ddim40', \n",
    "            'use_checkpoint':False,\n",
    "            'use_ddim':True, \n",
    "            'use_fp16':True, \n",
    "            'use_kl':False, \n",
    "            'use_new_attention_order':False, \n",
    "            'use_scale_shift_norm':True,\n",
    "            'data_dir':''})\n",
    "            \n",
    "    model_src, diffusion = create_model_and_diffusion(\n",
    "        **args_to_dict(args_src, model_and_diffusion_defaults().keys())\n",
    "    )\n",
    "\n",
    "    model_src.load_state_dict(\n",
    "        dist_util.load_state_dict(args_src.model_path, map_location=\"cpu\")\n",
    "    )\n",
    "    model_src.to(device)\n",
    "    model_src.convert_to_fp16()\n",
    "    model_src.eval()\n",
    "    print('done')\n",
    "\n",
    "    model_kwargs = {}\n",
    "    sample_fn = (\n",
    "        diffusion.p_sample_loop if not args_src.use_ddim else diffusion.ddim_sample_loop\n",
    "    )\n",
    "\n",
    "    n_trial = 3\n",
    "    for n in range(n_trial):\n",
    "        metric.reset()\n",
    "        metric.scope = f'SDEdit {n}th trial'\n",
    "        with th.no_grad():\n",
    "            for idx, batch in enumerate(data):\n",
    "                img, cond = batch\n",
    "                img = img.to(device)\n",
    "                noise = th.randn_like(img)\n",
    "                t = th.tensor([t_end] * img.shape[0], device=device)\n",
    "                perturbed = diffusion.q_sample(img, t, noise=noise)\n",
    "                sample_src = sample_fn(\n",
    "                model_src,\n",
    "                (args_src.batch_size, 3, args_src.image_size, args_src.image_size),\n",
    "                clip_denoised=args_src.clip_denoised,\n",
    "                model_kwargs=model_kwargs,\n",
    "                noise=noise,\n",
    "                resizers = (down, up),\n",
    "                range_t=20,\n",
    "                ref_img=img,\n",
    "                )\n",
    "                metric.update(img, sample_src)\n",
    "        \n",
    "        print(model_path)\n",
    "        print(model_path, file=f)\n",
    "        print(metric.print_metrics())\n",
    "        print(metric.print_metrics(), file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    for idx, batch in enumerate(data):\n",
    "        img, cond = batch\n",
    "        img = img.to(device)\n",
    "        noise = th.randn_like(img)\n",
    "        # t = th.tensor([t_end] * img.shape[0], device=device)\n",
    "        model_kwargs = {}\n",
    "        sample_src = sample_fn(\n",
    "            model_src,\n",
    "            (args_src.batch_size, 3, args_src.image_size, args_src.image_size),\n",
    "            clip_denoised=args_src.clip_denoised,\n",
    "            model_kwargs=model_kwargs,\n",
    "            noise=noise,\n",
    "            resizers = (down, up),\n",
    "            range_t=20,\n",
    "            ref_img=img,\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = ((img + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "pil_sam = ((sample_src + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "display(to_pil(pil_img[0]))\n",
    "display(to_pil(pil_sam[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90fa66dac3c1ddd348e3a72591d64bad417b44583f39858014c90b2ecbfdfc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
